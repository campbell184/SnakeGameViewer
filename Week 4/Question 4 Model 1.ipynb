{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Lab for Tutorial 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Bonus question - for bonus marks*. We will now look at a more challenging text-based classification problem, namely to classify a page from a Harry Potter book into which of the seven books the page was taken from. The books can be found in the zip file hp books.zip and are text files where each page of a given book is a line in the text file. Note, all punctuation and capital lett"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train an NB model using 80% of the data to train and the remaining 20% as test data. Use Laplace smoothing for your model. Report a confusion matrix of your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the data in a matrix\n",
    "files = ['hp_books\\\\HP1.txt', \n",
    "         'hp_books\\\\HP2.txt', \n",
    "         'hp_books\\\\HP3.txt', \n",
    "         'hp_books\\\\HP4.txt', \n",
    "         'hp_books\\\\HP5.txt', \n",
    "         'hp_books\\\\HP6.txt', \n",
    "         'hp_books\\\\HP7.txt']\n",
    "\n",
    "data = list()\n",
    "\n",
    "for file in files:\n",
    "    f = open(file)\n",
    "    line = f.readline()\n",
    "    \n",
    "    while(line):\n",
    "        data.append(line.rstrip('\\n') + file[11])\n",
    "        line = f.readline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select random 80% rows from data for training data\n",
    "train_data = np.random.choice(data, int(0.8*len(data)), replace=False)\n",
    "\n",
    "#the remaining 20% is for testing the model\n",
    "test_data = np.array([line for line in data if line not in train_data])\n",
    "\n",
    "#print('Train data:\\n')\n",
    "#print(train_data)\n",
    "\n",
    "#print('\\nTest data:\\n')\n",
    "#print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the traing data into x_train_data and y_train_label\n",
    "x_train_data = [line.split(' ')[:-1] for line in train_data]\n",
    "y_train_label = [line.split(' ')[-1] for line in train_data]\n",
    "y_train_label = np.array(y_train_label).astype(np.int32)\n",
    "\n",
    "#print('x Train data:\\n')\n",
    "#print(x_train_data)\n",
    "\n",
    "#print('\\ny Train label:\\n')\n",
    "#print(y_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selects all the features of every row in the training data\n",
    "features = list()\n",
    "for vec in x_train_data:\n",
    "    for word in vec:\n",
    "        if word not in features and word != '':\n",
    "            features.append(word)\n",
    "#print('All the Features of every data_point\\n')\n",
    "#print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prior Probability function\n",
    "def prior_prob(x):\n",
    "    number_of_x = 0\n",
    "    for label in y_train_label:\n",
    "        if label == x:\n",
    "            number_of_x += 1\n",
    "    return number_of_x/len(y_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conditional probability of x given that is class c\n",
    "def con_prob(x, c):\n",
    "    count = 0\n",
    "    number_of_x_and_c = 0\n",
    "    number_of_c = 0\n",
    "    \n",
    "    for vec in x_train_data:\n",
    "        if y_train_label[count] == c:\n",
    "            if x in vec:\n",
    "                number_of_x_and_c += 1\n",
    "            number_of_c += 1\n",
    "        count += 1\n",
    "    return number_of_x_and_c / number_of_c\n",
    "\n",
    "\n",
    "#Constructing Probability Table\n",
    "#prob_table stores ['word', con_prob(word, 1), con_prob(word, 2), ..., con_prob(word, 7)]\n",
    "prob_table = list()\n",
    "for word in features:\n",
    "    prob_table.append([word, \n",
    "                        con_prob(word, 1), \n",
    "                        con_prob(word, 2),\n",
    "                        con_prob(word, 3),\n",
    "                        con_prob(word, 4),\n",
    "                        con_prob(word, 5),\n",
    "                        con_prob(word, 6),\n",
    "                        con_prob(word, 7)])\n",
    "    #print('I am still working.....')\n",
    "\n",
    "#print(prob_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Adapt your code to use 80% of the data to train, 10% of the data as validation data and the remaining 10% as test data. Train separate NB classifiers using the values {1 × 10−1 , 1 × 10−2 , 1 × 10−3 , 1 × 10−4 , 1×10−5 , 1×10−6} to smooth the table of likelihoods. Train each model using the training data, and track its performance on the validation data. Which model gave the best accuracy on validation data? Does the choice of smoothing value have a big impact on the performance of the model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new way for smoothing the data\n",
    "#inputs: probability table, hyperparameter\n",
    "def updated_prob_table(param):\n",
    "    for vec in prob_table:\n",
    "        for i in range(1, 8):\n",
    "            vec[i] += param\n",
    "    \n",
    "\n",
    "#Training the NB Model\n",
    "#input : array data_point\n",
    "def Predicted_class(data_point, param):\n",
    "    \n",
    "    d = dict()\n",
    "    class_labels = np.arange(1, 8)\n",
    "    \n",
    "    #likelihood probabilies\n",
    "    for label in class_labels:\n",
    "        d[label] = 0\n",
    "    \n",
    "    #word doesn't exist in features\n",
    "    for word in data_point:\n",
    "        if word not in features:\n",
    "            features.append(word)\n",
    "            l = [word]\n",
    "            for label in d:\n",
    "                temp = param\n",
    "                l.append(temp)\n",
    "            updated_prob_table(param)\n",
    "            prob_table.append(l)\n",
    "    \n",
    "    #computing the probabilities\n",
    "    for word in features:\n",
    "        \n",
    "        i = features.index(word)\n",
    "        if word in data_point:\n",
    "            for label in d:\n",
    "                \n",
    "                if prob(prob_table[i][label]) == 0:\n",
    "                    \n",
    "                    prob_table = updated_prob_table(prob_table, param)\n",
    "                    d[label] += math.log(prob(prob_table[i][label]))\n",
    "                    \n",
    "                else:\n",
    "                    d[label] += math.log(prob(prob_table[i][label]))\n",
    "        \n",
    "        else:\n",
    "            for label in d:\n",
    "                \n",
    "                if prob(prob_table[i][label][0]) == 1:\n",
    "                    \n",
    "                    prob_table = updated_prob_table(prob_table, param)\n",
    "                    d[label] += math.log(1 - prob(prob_table[i][label]))\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    d[label] += math.log(1 - prob(prob_table[i][label]))\n",
    "                \n",
    "                \n",
    "    denominator = 0\n",
    "    \n",
    "    \n",
    "    #store the prior probabilities\n",
    "    prior_probabilities = list([1])\n",
    "        \n",
    "    \n",
    "    for label in d:\n",
    "        prior_probabilities.append(prior_prob(label))\n",
    "        denominator += math.exp(d[label]) * prior_probabilites[label]\n",
    "    \n",
    "    \n",
    "    for label in d:\n",
    "        if denominator == 0: denominator = 1\n",
    "        d[label] = math.exp(d[label] + math.log(prior_probabilities[label]) - math.log(denominator))\n",
    "    \n",
    "    \n",
    "    return [label for label in d.keys() if d[label] == max(d.values())][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting the test data into validation and testing data. 10% validation, 10% testing from the 20%\n",
    "validate_data = np.random.choice(test_data, int(0.5*len(test_data)), replace=False)\n",
    "\n",
    "#the remaining 10% is for testing the model\n",
    "test_data = np.array([line for line in test_data if line not in validate_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'prob_table' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-118-dc8a290a0260>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvec\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx_validate_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0my_Predicted_label\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPredicted_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNB_Model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-116-0dde96a088d3>\u001b[0m in \u001b[0;36mPredicted_class\u001b[1;34m(data_point, param)\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mupdated_prob_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mprob_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m#computing the probabilities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'prob_table' referenced before assignment"
     ]
    }
   ],
   "source": [
    "#Validating the models of NB\n",
    "#NB hyperparameters for the models\n",
    "NB_Model = pow(10, -1)\n",
    "\n",
    "\n",
    "#validation data\n",
    "x_validate_data = [line.split(' ')[:-1] for line in validate_data]\n",
    "y_validate_label = [line.split(' ')[-1] for line in validate_data]\n",
    "#print(y_test_label)\n",
    "\n",
    "    \n",
    "y_Predicted_label = list()\n",
    "count = 0\n",
    "for vec in x_validate_data:\n",
    "    y_Predicted_label.append(Predicted_class(vec, NB_Model))\n",
    "    count += 1\n",
    "    print(count)\n",
    "#print(y_Predicted_label)\n",
    "\n",
    "\n",
    "#computing confusion matrix\n",
    "class_labels = [1, 2, 3, 4, 5, 6, 7]\n",
    "confusion_matrix = list()\n",
    "\n",
    "count = 0\n",
    "\n",
    "for label1 in class_labels:\n",
    "    l = list()\n",
    "    for label2 in class_labels:\n",
    "        for i in range(len(y_validate_label)):\n",
    "            if y_Predicted_label[i] == label2 and y_validate_label[i] == label1:\n",
    "                count += 1\n",
    "        l.append(count)\n",
    "        count = 0\n",
    "    confusion_matrix.append(l)\n",
    "    l = list()\n",
    "print('Confusion Matrix of model ' + str(NB_Model) + '\\n')\n",
    "print(confusion_matrix)\n",
    "\n",
    "\n",
    "\n",
    "count, count_correct_match = 0, 0\n",
    "for label in y_Predicted_label:\n",
    "    if label == y_validate_label[count]:\n",
    "        count_correct_match += 1\n",
    "    count += 1\n",
    "\n",
    "accuracy = count_correct_match / count\n",
    "\n",
    "print('\\nAccuracy of the Model: ', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Use the model which achieved the best validation accuracy and test it using the test data set. Report a confusion matrix of the results, as well as the test accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the best model with the test data\n",
    "x_test_data = [line.split(' ')[:-1] for line in test_data]\n",
    "y_test_label = [line.split(' ')[-1] for line in test_data]\n",
    "y_Predicted_label = list()\n",
    "#print(y_test_label)\n",
    "\n",
    "for vec in x_test_data:\n",
    "    y_Predicted_label.append(Predicted_Class(vec, Best_Model))\n",
    "#print(y_Predicted_label)\n",
    "\n",
    "\n",
    "#computing confusion matrix\n",
    "class_labels = [1, 2, 3, 4, 5, 6, 7]\n",
    "confusion_matrix = list()\n",
    "\n",
    "count = 0\n",
    "\n",
    "for label1 in class_labels:\n",
    "    l = list()\n",
    "    for label2 in class_labels:\n",
    "        for i in range(len(y_test_label)):\n",
    "            if y_Predicted_label[i] == label2 and y_test_label[i] == label1:\n",
    "                count += 1\n",
    "        l.append(count)\n",
    "        count = 0\n",
    "    confusion_matrix.append(l)\n",
    "    l = list()\n",
    "print('Confusion Matrix of the Best Model\\n')\n",
    "print(confusion_matrix)\n",
    "\n",
    "\n",
    "\n",
    "count, count_correct_match = 0, 0\n",
    "for label in y_Predicted_label:\n",
    "    if label == y_test_label[count]:\n",
    "        count_correct_match += 1\n",
    "    count += 1\n",
    "    \n",
    "\n",
    "print('\\nAccuracy of the Model: ', count_correct_match / count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
